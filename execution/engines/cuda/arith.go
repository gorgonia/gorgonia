package cuda

import (
	"context"
	"unsafe"

	"gorgonia.org/gorgonia/internal/errors"
	"gorgonia.org/internal/debug"
	"gorgonia.org/tensor"
)

// Code generated by gencudaengine, which is a API generation tool for Gorgonia. DO NOT EDIT.

// Add implements tensor.Adder. It does not support safe or increment operation options and will return an error if those options are passed in.
func (e *Engine[DT, T]) Add(ctx context.Context, a, b, retVal T, toIncr bool) (err error) {
	name, _, _ := constructBinName2(a, b, "add", false)

	if err = binaryCheck[DT](a, b); err != nil {
		return errors.Wrap(err, "Basic checks failed for Add")
	}
	mem, memB, size := e.opMem(a, b, retVal)

	debug.Logf("CUDADO %q, Mem: %v MemB: %v size %v", name, mem, memB, size)
	debug.Logf("LaunchKernel Params. mem: %v. Size %v", mem, size)
	if err = e.Call(name, int(size), unsafe.Pointer(&mem), unsafe.Pointer(&memB), unsafe.Pointer(&size)); err != nil {
		err = errors.Wrap(err, "Unable to perform engine.Add - CUDA LaunchAndSync failed.")
	}
	return
}

// AddScalar implements tensor.Adder. It does not support safe or increment operation options and will return an error if those options are passed in.
func (e *Engine[DT, T]) AddScalar(ctx context.Context, a T, b DT, retVal T, leftTensor, toIncr bool) (err error) {
	return errors.NYI()
	/*
		name := constructBinName1(a, leftTensor, "add")

		var bMem tensor.Memory
		var ok bool
		if bMem, ok = b.(tensor.Memory); !ok {
			return errors.Errorf("b has to be a tensor.Memory. Got %T instead", b)
		}

		if err = unaryCheck[DT](a); err != nil {
			return errors.Wrap(err, "Basic checks failed for AddScalar")
		}


		var mem, memB cu.DevicePtr
		var size int64
		if mem, size, retVal, err = e.opMem(a, opts...); err != nil{
			return errors.Wrap(err, "Unable to perform Add")
		}
		memB = cu.DevicePtr(bMem.Uintptr())
		if !leftTensor {
			mem, memB = memB, mem
		}

		debug.Logf("CUDADO %q, Mem: %v size %v, args %v", name, mem, size)
		debug.Logf("LaunchKernel Params. mem: %v. Size %v", mem, size)
		if err = e.Call(name, int(size), unsafe.Pointer(&mem), unsafe.Pointer(&memB), unsafe.Pointer(&size)); err != nil{
			err = errors.Wrap(err, "Unable to perform engine.Add - CUDA LaunchAndSync failed.")
		}
		return
	*/
}

func (e *Engine[DT, T]) AddBroadcastable(ctx context.Context, a, b, retVal T, expAPA, expAPB *tensor.AP, toIncr bool) (err error) {
	// check if it's a scalar in a or b
	name, scalarOnLeft, scalarOnRight := constructBinName2(a, b, "add", true)
	isScalar := scalarOnLeft || scalarOnRight
	// scalar
	if isScalar {
		var t T
		if scalarOnLeft {
			t = b
		} else {
			t = a
		}
		if err = unaryCheck[DT](t); err != nil {
			return errors.Wrap(err, "Basic checks failed for AddBroadcastable")
		}
		mem, memB, size := e.opMem(a, b, retVal)
		if scalarOnLeft {
			mem, memB = memB, mem
		}

		debug.Logf("CUDADO %q, Mem: %v MemB: %v size %v", name, mem, memB, size)
		debug.Logf("LaunchKernel Params. mem: %v. Size %v", mem, size)
		if err = e.Call(name, int(size), unsafe.Pointer(&mem), unsafe.Pointer(&memB), unsafe.Pointer(&size)); err != nil {
			err = errors.Wrap(err, "Unable to perform engine.Add - CUDA LaunchAndSync failed.")
		}
		return
	}

	sp, totalAlloc, err := e.prepShapes(expAPA, expAPB, retVal)
	if err != nil {
		return errors.Wrap(err, "Failed to prep shapes")
	}
	_ = totalAlloc
	// TODO: sp is a slice of CUDA memory. They need to be freed. Add to this once the hook architecture is finished in package cu.

	mem, memB, memRetVal, size := e.opMemBC(a, b, retVal)
	debug.Logf("CUDADO %q, Mem: %v MemB: %v size %v", name, mem, memB, size)
	debug.Logf("LaunchKernel Params. mem: %v. Size %v", mem, size)
	if err = e.Call(name, int(size), unsafe.Pointer(&mem), unsafe.Pointer(&memB), unsafe.Pointer(&memRetVal),
		unsafe.Pointer(&sp[0]), unsafe.Pointer(&sp[1]), unsafe.Pointer(&sp[2]),
		unsafe.Pointer(&sp[3]), unsafe.Pointer(&sp[4]), unsafe.Pointer(&size)); err != nil {
		err = errors.Wrap(err, "Unable to perform engine.AddBC - CUDA LaunchAndSync failed.")
	}

	return

}

// Sub implements tensor.Suber. It does not support safe or increment operation options and will return an error if those options are passed in.
func (e *Engine[DT, T]) Sub(ctx context.Context, a, b, retVal T, toIncr bool) (err error) {
	name, _, _ := constructBinName2(a, b, "sub", false)

	if err = binaryCheck[DT](a, b); err != nil {
		return errors.Wrap(err, "Basic checks failed for Sub")
	}
	mem, memB, size := e.opMem(a, b, retVal)

	debug.Logf("CUDADO %q, Mem: %v MemB: %v size %v", name, mem, memB, size)
	debug.Logf("LaunchKernel Params. mem: %v. Size %v", mem, size)
	if err = e.Call(name, int(size), unsafe.Pointer(&mem), unsafe.Pointer(&memB), unsafe.Pointer(&size)); err != nil {
		err = errors.Wrap(err, "Unable to perform engine.Sub - CUDA LaunchAndSync failed.")
	}
	return
}

// SubScalar implements tensor.Suber. It does not support safe or increment operation options and will return an error if those options are passed in.
func (e *Engine[DT, T]) SubScalar(ctx context.Context, a T, b DT, retVal T, leftTensor, toIncr bool) (err error) {
	return errors.NYI()
	/*
		name := constructBinName1(a, leftTensor, "sub")

		var bMem tensor.Memory
		var ok bool
		if bMem, ok = b.(tensor.Memory); !ok {
			return errors.Errorf("b has to be a tensor.Memory. Got %T instead", b)
		}

		if err = unaryCheck[DT](a); err != nil {
			return errors.Wrap(err, "Basic checks failed for SubScalar")
		}


		var mem, memB cu.DevicePtr
		var size int64
		if mem, size, retVal, err = e.opMem(a, opts...); err != nil{
			return errors.Wrap(err, "Unable to perform Sub")
		}
		memB = cu.DevicePtr(bMem.Uintptr())
		if !leftTensor {
			mem, memB = memB, mem
		}

		debug.Logf("CUDADO %q, Mem: %v size %v, args %v", name, mem, size)
		debug.Logf("LaunchKernel Params. mem: %v. Size %v", mem, size)
		if err = e.Call(name, int(size), unsafe.Pointer(&mem), unsafe.Pointer(&memB), unsafe.Pointer(&size)); err != nil{
			err = errors.Wrap(err, "Unable to perform engine.Sub - CUDA LaunchAndSync failed.")
		}
		return
	*/
}

func (e *Engine[DT, T]) SubBroadcastable(ctx context.Context, a, b, retVal T, expAPA, expAPB *tensor.AP, toIncr bool) (err error) {
	// check if it's a scalar in a or b
	name, scalarOnLeft, scalarOnRight := constructBinName2(a, b, "sub", true)
	isScalar := scalarOnLeft || scalarOnRight
	// scalar
	if isScalar {
		var t T
		if scalarOnLeft {
			t = b
		} else {
			t = a
		}
		if err = unaryCheck[DT](t); err != nil {
			return errors.Wrap(err, "Basic checks failed for SubBroadcastable")
		}
		mem, memB, size := e.opMem(a, b, retVal)
		if scalarOnLeft {
			mem, memB = memB, mem
		}

		debug.Logf("CUDADO %q, Mem: %v MemB: %v size %v", name, mem, memB, size)
		debug.Logf("LaunchKernel Params. mem: %v. Size %v", mem, size)
		if err = e.Call(name, int(size), unsafe.Pointer(&mem), unsafe.Pointer(&memB), unsafe.Pointer(&size)); err != nil {
			err = errors.Wrap(err, "Unable to perform engine.Add - CUDA LaunchAndSync failed.")
		}
		return
	}

	sp, totalAlloc, err := e.prepShapes(expAPA, expAPB, retVal)
	if err != nil {
		return errors.Wrap(err, "Failed to prep shapes")
	}
	_ = totalAlloc
	// TODO: sp is a slice of CUDA memory. They need to be freed. Add to this once the hook architecture is finished in package cu.

	mem, memB, memRetVal, size := e.opMemBC(a, b, retVal)
	debug.Logf("CUDADO %q, Mem: %v MemB: %v size %v", name, mem, memB, size)
	debug.Logf("LaunchKernel Params. mem: %v. Size %v", mem, size)
	if err = e.Call(name, int(size), unsafe.Pointer(&mem), unsafe.Pointer(&memB), unsafe.Pointer(&memRetVal),
		unsafe.Pointer(&sp[0]), unsafe.Pointer(&sp[1]), unsafe.Pointer(&sp[2]),
		unsafe.Pointer(&sp[3]), unsafe.Pointer(&sp[4]), unsafe.Pointer(&size)); err != nil {
		err = errors.Wrap(err, "Unable to perform engine.AddBC - CUDA LaunchAndSync failed.")
	}

	return

}

// Mul implements tensor.Muler. It does not support safe or increment operation options and will return an error if those options are passed in.
func (e *Engine[DT, T]) Mul(ctx context.Context, a, b, retVal T, toIncr bool) (err error) {
	name, _, _ := constructBinName2(a, b, "mul", false)

	if err = binaryCheck[DT](a, b); err != nil {
		return errors.Wrap(err, "Basic checks failed for Mul")
	}
	mem, memB, size := e.opMem(a, b, retVal)

	debug.Logf("CUDADO %q, Mem: %v MemB: %v size %v", name, mem, memB, size)
	debug.Logf("LaunchKernel Params. mem: %v. Size %v", mem, size)
	if err = e.Call(name, int(size), unsafe.Pointer(&mem), unsafe.Pointer(&memB), unsafe.Pointer(&size)); err != nil {
		err = errors.Wrap(err, "Unable to perform engine.Mul - CUDA LaunchAndSync failed.")
	}
	return
}

// MulScalar implements tensor.Muler. It does not support safe or increment operation options and will return an error if those options are passed in.
func (e *Engine[DT, T]) MulScalar(ctx context.Context, a T, b DT, retVal T, leftTensor, toIncr bool) (err error) {
	return errors.NYI()
	/*
		name := constructBinName1(a, leftTensor, "mul")

		var bMem tensor.Memory
		var ok bool
		if bMem, ok = b.(tensor.Memory); !ok {
			return errors.Errorf("b has to be a tensor.Memory. Got %T instead", b)
		}

		if err = unaryCheck[DT](a); err != nil {
			return errors.Wrap(err, "Basic checks failed for MulScalar")
		}


		var mem, memB cu.DevicePtr
		var size int64
		if mem, size, retVal, err = e.opMem(a, opts...); err != nil{
			return errors.Wrap(err, "Unable to perform Mul")
		}
		memB = cu.DevicePtr(bMem.Uintptr())
		if !leftTensor {
			mem, memB = memB, mem
		}

		debug.Logf("CUDADO %q, Mem: %v size %v, args %v", name, mem, size)
		debug.Logf("LaunchKernel Params. mem: %v. Size %v", mem, size)
		if err = e.Call(name, int(size), unsafe.Pointer(&mem), unsafe.Pointer(&memB), unsafe.Pointer(&size)); err != nil{
			err = errors.Wrap(err, "Unable to perform engine.Mul - CUDA LaunchAndSync failed.")
		}
		return
	*/
}

func (e *Engine[DT, T]) MulBroadcastable(ctx context.Context, a, b, retVal T, expAPA, expAPB *tensor.AP, toIncr bool) (err error) {
	// check if it's a scalar in a or b
	name, scalarOnLeft, scalarOnRight := constructBinName2(a, b, "mul", true)
	isScalar := scalarOnLeft || scalarOnRight
	// scalar
	if isScalar {
		var t T
		if scalarOnLeft {
			t = b
		} else {
			t = a
		}
		if err = unaryCheck[DT](t); err != nil {
			return errors.Wrap(err, "Basic checks failed for MulBroadcastable")
		}
		mem, memB, size := e.opMem(a, b, retVal)
		if scalarOnLeft {
			mem, memB = memB, mem
		}

		debug.Logf("CUDADO %q, Mem: %v MemB: %v size %v", name, mem, memB, size)
		debug.Logf("LaunchKernel Params. mem: %v. Size %v", mem, size)
		if err = e.Call(name, int(size), unsafe.Pointer(&mem), unsafe.Pointer(&memB), unsafe.Pointer(&size)); err != nil {
			err = errors.Wrap(err, "Unable to perform engine.Add - CUDA LaunchAndSync failed.")
		}
		return
	}

	sp, totalAlloc, err := e.prepShapes(expAPA, expAPB, retVal)
	if err != nil {
		return errors.Wrap(err, "Failed to prep shapes")
	}
	_ = totalAlloc
	// TODO: sp is a slice of CUDA memory. They need to be freed. Add to this once the hook architecture is finished in package cu.

	mem, memB, memRetVal, size := e.opMemBC(a, b, retVal)
	debug.Logf("CUDADO %q, Mem: %v MemB: %v size %v", name, mem, memB, size)
	debug.Logf("LaunchKernel Params. mem: %v. Size %v", mem, size)
	if err = e.Call(name, int(size), unsafe.Pointer(&mem), unsafe.Pointer(&memB), unsafe.Pointer(&memRetVal),
		unsafe.Pointer(&sp[0]), unsafe.Pointer(&sp[1]), unsafe.Pointer(&sp[2]),
		unsafe.Pointer(&sp[3]), unsafe.Pointer(&sp[4]), unsafe.Pointer(&size)); err != nil {
		err = errors.Wrap(err, "Unable to perform engine.AddBC - CUDA LaunchAndSync failed.")
	}

	return

}

// Div implements tensor.Diver. It does not support safe or increment operation options and will return an error if those options are passed in.
func (e *Engine[DT, T]) Div(ctx context.Context, a, b, retVal T, toIncr bool) (err error) {
	name, _, _ := constructBinName2(a, b, "div", false)

	if err = binaryCheck[DT](a, b); err != nil {
		return errors.Wrap(err, "Basic checks failed for Div")
	}
	mem, memB, size := e.opMem(a, b, retVal)

	debug.Logf("CUDADO %q, Mem: %v MemB: %v size %v", name, mem, memB, size)
	debug.Logf("LaunchKernel Params. mem: %v. Size %v", mem, size)
	if err = e.Call(name, int(size), unsafe.Pointer(&mem), unsafe.Pointer(&memB), unsafe.Pointer(&size)); err != nil {
		err = errors.Wrap(err, "Unable to perform engine.Div - CUDA LaunchAndSync failed.")
	}
	return
}

// DivScalar implements tensor.Diver. It does not support safe or increment operation options and will return an error if those options are passed in.
func (e *Engine[DT, T]) DivScalar(ctx context.Context, a T, b DT, retVal T, leftTensor, toIncr bool) (err error) {
	return errors.NYI()
	/*
		name := constructBinName1(a, leftTensor, "div")

		var bMem tensor.Memory
		var ok bool
		if bMem, ok = b.(tensor.Memory); !ok {
			return errors.Errorf("b has to be a tensor.Memory. Got %T instead", b)
		}

		if err = unaryCheck[DT](a); err != nil {
			return errors.Wrap(err, "Basic checks failed for DivScalar")
		}


		var mem, memB cu.DevicePtr
		var size int64
		if mem, size, retVal, err = e.opMem(a, opts...); err != nil{
			return errors.Wrap(err, "Unable to perform Div")
		}
		memB = cu.DevicePtr(bMem.Uintptr())
		if !leftTensor {
			mem, memB = memB, mem
		}

		debug.Logf("CUDADO %q, Mem: %v size %v, args %v", name, mem, size)
		debug.Logf("LaunchKernel Params. mem: %v. Size %v", mem, size)
		if err = e.Call(name, int(size), unsafe.Pointer(&mem), unsafe.Pointer(&memB), unsafe.Pointer(&size)); err != nil{
			err = errors.Wrap(err, "Unable to perform engine.Div - CUDA LaunchAndSync failed.")
		}
		return
	*/
}

func (e *Engine[DT, T]) DivBroadcastable(ctx context.Context, a, b, retVal T, expAPA, expAPB *tensor.AP, toIncr bool) (err error) {
	// check if it's a scalar in a or b
	name, scalarOnLeft, scalarOnRight := constructBinName2(a, b, "div", true)
	isScalar := scalarOnLeft || scalarOnRight
	// scalar
	if isScalar {
		var t T
		if scalarOnLeft {
			t = b
		} else {
			t = a
		}
		if err = unaryCheck[DT](t); err != nil {
			return errors.Wrap(err, "Basic checks failed for DivBroadcastable")
		}
		mem, memB, size := e.opMem(a, b, retVal)
		if scalarOnLeft {
			mem, memB = memB, mem
		}

		debug.Logf("CUDADO %q, Mem: %v MemB: %v size %v", name, mem, memB, size)
		debug.Logf("LaunchKernel Params. mem: %v. Size %v", mem, size)
		if err = e.Call(name, int(size), unsafe.Pointer(&mem), unsafe.Pointer(&memB), unsafe.Pointer(&size)); err != nil {
			err = errors.Wrap(err, "Unable to perform engine.Add - CUDA LaunchAndSync failed.")
		}
		return
	}

	sp, totalAlloc, err := e.prepShapes(expAPA, expAPB, retVal)
	if err != nil {
		return errors.Wrap(err, "Failed to prep shapes")
	}
	_ = totalAlloc
	// TODO: sp is a slice of CUDA memory. They need to be freed. Add to this once the hook architecture is finished in package cu.

	mem, memB, memRetVal, size := e.opMemBC(a, b, retVal)
	debug.Logf("CUDADO %q, Mem: %v MemB: %v size %v", name, mem, memB, size)
	debug.Logf("LaunchKernel Params. mem: %v. Size %v", mem, size)
	if err = e.Call(name, int(size), unsafe.Pointer(&mem), unsafe.Pointer(&memB), unsafe.Pointer(&memRetVal),
		unsafe.Pointer(&sp[0]), unsafe.Pointer(&sp[1]), unsafe.Pointer(&sp[2]),
		unsafe.Pointer(&sp[3]), unsafe.Pointer(&sp[4]), unsafe.Pointer(&size)); err != nil {
		err = errors.Wrap(err, "Unable to perform engine.AddBC - CUDA LaunchAndSync failed.")
	}

	return

}

// Pow implements tensor.Power. It does not support safe or increment operation options and will return an error if those options are passed in.
func (e *Engine[DT, T]) Pow(ctx context.Context, a, b, retVal T, toIncr bool) (err error) {
	name, _, _ := constructBinName2(a, b, "pow", false)

	if err = binaryCheck[DT](a, b); err != nil {
		return errors.Wrap(err, "Basic checks failed for Pow")
	}
	mem, memB, size := e.opMem(a, b, retVal)

	debug.Logf("CUDADO %q, Mem: %v MemB: %v size %v", name, mem, memB, size)
	debug.Logf("LaunchKernel Params. mem: %v. Size %v", mem, size)
	if err = e.Call(name, int(size), unsafe.Pointer(&mem), unsafe.Pointer(&memB), unsafe.Pointer(&size)); err != nil {
		err = errors.Wrap(err, "Unable to perform engine.Pow - CUDA LaunchAndSync failed.")
	}
	return
}

// PowScalar implements tensor.Power. It does not support safe or increment operation options and will return an error if those options are passed in.
func (e *Engine[DT, T]) PowScalar(ctx context.Context, a T, b DT, retVal T, leftTensor, toIncr bool) (err error) {
	return errors.NYI()
	/*
		name := constructBinName1(a, leftTensor, "pow")

		var bMem tensor.Memory
		var ok bool
		if bMem, ok = b.(tensor.Memory); !ok {
			return errors.Errorf("b has to be a tensor.Memory. Got %T instead", b)
		}

		if err = unaryCheck[DT](a); err != nil {
			return errors.Wrap(err, "Basic checks failed for PowScalar")
		}


		var mem, memB cu.DevicePtr
		var size int64
		if mem, size, retVal, err = e.opMem(a, opts...); err != nil{
			return errors.Wrap(err, "Unable to perform Pow")
		}
		memB = cu.DevicePtr(bMem.Uintptr())
		if !leftTensor {
			mem, memB = memB, mem
		}

		debug.Logf("CUDADO %q, Mem: %v size %v, args %v", name, mem, size)
		debug.Logf("LaunchKernel Params. mem: %v. Size %v", mem, size)
		if err = e.Call(name, int(size), unsafe.Pointer(&mem), unsafe.Pointer(&memB), unsafe.Pointer(&size)); err != nil{
			err = errors.Wrap(err, "Unable to perform engine.Pow - CUDA LaunchAndSync failed.")
		}
		return
	*/
}

func (e *Engine[DT, T]) PowBroadcastable(ctx context.Context, a, b, retVal T, expAPA, expAPB *tensor.AP, toIncr bool) (err error) {
	// check if it's a scalar in a or b
	name, scalarOnLeft, scalarOnRight := constructBinName2(a, b, "pow", true)
	isScalar := scalarOnLeft || scalarOnRight
	// scalar
	if isScalar {
		var t T
		if scalarOnLeft {
			t = b
		} else {
			t = a
		}
		if err = unaryCheck[DT](t); err != nil {
			return errors.Wrap(err, "Basic checks failed for PowBroadcastable")
		}
		mem, memB, size := e.opMem(a, b, retVal)
		if scalarOnLeft {
			mem, memB = memB, mem
		}

		debug.Logf("CUDADO %q, Mem: %v MemB: %v size %v", name, mem, memB, size)
		debug.Logf("LaunchKernel Params. mem: %v. Size %v", mem, size)
		if err = e.Call(name, int(size), unsafe.Pointer(&mem), unsafe.Pointer(&memB), unsafe.Pointer(&size)); err != nil {
			err = errors.Wrap(err, "Unable to perform engine.Add - CUDA LaunchAndSync failed.")
		}
		return
	}

	sp, totalAlloc, err := e.prepShapes(expAPA, expAPB, retVal)
	if err != nil {
		return errors.Wrap(err, "Failed to prep shapes")
	}
	_ = totalAlloc
	// TODO: sp is a slice of CUDA memory. They need to be freed. Add to this once the hook architecture is finished in package cu.

	mem, memB, memRetVal, size := e.opMemBC(a, b, retVal)
	debug.Logf("CUDADO %q, Mem: %v MemB: %v size %v", name, mem, memB, size)
	debug.Logf("LaunchKernel Params. mem: %v. Size %v", mem, size)
	if err = e.Call(name, int(size), unsafe.Pointer(&mem), unsafe.Pointer(&memB), unsafe.Pointer(&memRetVal),
		unsafe.Pointer(&sp[0]), unsafe.Pointer(&sp[1]), unsafe.Pointer(&sp[2]),
		unsafe.Pointer(&sp[3]), unsafe.Pointer(&sp[4]), unsafe.Pointer(&size)); err != nil {
		err = errors.Wrap(err, "Unable to perform engine.AddBC - CUDA LaunchAndSync failed.")
	}

	return

}

// Mod implements tensor.Moder. It does not support safe or increment operation options and will return an error if those options are passed in.
func (e *Engine[DT, T]) Mod(ctx context.Context, a, b, retVal T, toIncr bool) (err error) {
	name, _, _ := constructBinName2(a, b, "mod", false)

	if err = binaryCheck[DT](a, b); err != nil {
		return errors.Wrap(err, "Basic checks failed for Mod")
	}
	mem, memB, size := e.opMem(a, b, retVal)

	debug.Logf("CUDADO %q, Mem: %v MemB: %v size %v", name, mem, memB, size)
	debug.Logf("LaunchKernel Params. mem: %v. Size %v", mem, size)
	if err = e.Call(name, int(size), unsafe.Pointer(&mem), unsafe.Pointer(&memB), unsafe.Pointer(&size)); err != nil {
		err = errors.Wrap(err, "Unable to perform engine.Mod - CUDA LaunchAndSync failed.")
	}
	return
}

// ModScalar implements tensor.Moder. It does not support safe or increment operation options and will return an error if those options are passed in.
func (e *Engine[DT, T]) ModScalar(ctx context.Context, a T, b DT, retVal T, leftTensor, toIncr bool) (err error) {
	return errors.NYI()
	/*
		name := constructBinName1(a, leftTensor, "mod")

		var bMem tensor.Memory
		var ok bool
		if bMem, ok = b.(tensor.Memory); !ok {
			return errors.Errorf("b has to be a tensor.Memory. Got %T instead", b)
		}

		if err = unaryCheck[DT](a); err != nil {
			return errors.Wrap(err, "Basic checks failed for ModScalar")
		}


		var mem, memB cu.DevicePtr
		var size int64
		if mem, size, retVal, err = e.opMem(a, opts...); err != nil{
			return errors.Wrap(err, "Unable to perform Mod")
		}
		memB = cu.DevicePtr(bMem.Uintptr())
		if !leftTensor {
			mem, memB = memB, mem
		}

		debug.Logf("CUDADO %q, Mem: %v size %v, args %v", name, mem, size)
		debug.Logf("LaunchKernel Params. mem: %v. Size %v", mem, size)
		if err = e.Call(name, int(size), unsafe.Pointer(&mem), unsafe.Pointer(&memB), unsafe.Pointer(&size)); err != nil{
			err = errors.Wrap(err, "Unable to perform engine.Mod - CUDA LaunchAndSync failed.")
		}
		return
	*/
}

func (e *Engine[DT, T]) ModBroadcastable(ctx context.Context, a, b, retVal T, expAPA, expAPB *tensor.AP, toIncr bool) (err error) {
	// check if it's a scalar in a or b
	name, scalarOnLeft, scalarOnRight := constructBinName2(a, b, "mod", true)
	isScalar := scalarOnLeft || scalarOnRight
	// scalar
	if isScalar {
		var t T
		if scalarOnLeft {
			t = b
		} else {
			t = a
		}
		if err = unaryCheck[DT](t); err != nil {
			return errors.Wrap(err, "Basic checks failed for ModBroadcastable")
		}
		mem, memB, size := e.opMem(a, b, retVal)
		if scalarOnLeft {
			mem, memB = memB, mem
		}

		debug.Logf("CUDADO %q, Mem: %v MemB: %v size %v", name, mem, memB, size)
		debug.Logf("LaunchKernel Params. mem: %v. Size %v", mem, size)
		if err = e.Call(name, int(size), unsafe.Pointer(&mem), unsafe.Pointer(&memB), unsafe.Pointer(&size)); err != nil {
			err = errors.Wrap(err, "Unable to perform engine.Add - CUDA LaunchAndSync failed.")
		}
		return
	}

	sp, totalAlloc, err := e.prepShapes(expAPA, expAPB, retVal)
	if err != nil {
		return errors.Wrap(err, "Failed to prep shapes")
	}
	_ = totalAlloc
	// TODO: sp is a slice of CUDA memory. They need to be freed. Add to this once the hook architecture is finished in package cu.

	mem, memB, memRetVal, size := e.opMemBC(a, b, retVal)
	debug.Logf("CUDADO %q, Mem: %v MemB: %v size %v", name, mem, memB, size)
	debug.Logf("LaunchKernel Params. mem: %v. Size %v", mem, size)
	if err = e.Call(name, int(size), unsafe.Pointer(&mem), unsafe.Pointer(&memB), unsafe.Pointer(&memRetVal),
		unsafe.Pointer(&sp[0]), unsafe.Pointer(&sp[1]), unsafe.Pointer(&sp[2]),
		unsafe.Pointer(&sp[3]), unsafe.Pointer(&sp[4]), unsafe.Pointer(&size)); err != nil {
		err = errors.Wrap(err, "Unable to perform engine.AddBC - CUDA LaunchAndSync failed.")
	}

	return

}
