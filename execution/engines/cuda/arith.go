package cuda

import (
	"unsafe"

	"github.com/pkg/errors"
	"gorgonia.org/cu"
	"gorgonia.org/internal/debug"
	"gorgonia.org/tensor"
)

// Code generated by gencudaengine, which is a API generation tool for Gorgonia. DO NOT EDIT.

// Add implements tensor.Adder. It does not support safe or increment operation options and will return an error if those options are passed in.
func (e *Engine) Add(a tensor.Tensor, b tensor.Tensor, opts ...tensor.FuncOpt) (retVal tensor.Tensor, err error) {
	name := constructBinName2(a, b, "add")

	if err = binaryCheck(a, b); err != nil {
		return nil, errors.Wrap(err, "Basic checks failed for Add")
	}

	var mem, memB cu.DevicePtr
	var size int64
	if mem, size, retVal, err = e.opMem(a, opts...); err != nil {
		return nil, errors.Wrap(err, "Unable to perform Add")
	}
	memB = cu.DevicePtr(b.Uintptr())

	debug.Logf("CUDADO %q, Mem: %v MemB: %v size %v", name, mem, memB, size)
	debug.Logf("LaunchKernel Params. mem: %v. Size %v", mem, size)
	if err = e.Call(name, int(size), unsafe.Pointer(&mem), unsafe.Pointer(&memB), unsafe.Pointer(&size)); err != nil {
		err = errors.Wrap(err, "Unable to perform engine.Add - CUDA LaunchAndSync failed.")
	}
	return
}

// AddScalar implements tensor.Adder. It does not support safe or increment operation options and will return an error if those options are passed in.
func (e *Engine) AddScalar(a tensor.Tensor, b interface{}, leftTensor bool, opts ...tensor.FuncOpt) (retVal tensor.Tensor, err error) {
	name := constructBinName1(a, leftTensor, "add")

	var bMem tensor.Memory
	var ok bool
	if bMem, ok = b.(tensor.Memory); !ok {
		return nil, errors.Errorf("b has to be a tensor.Memory. Got %T instead", b)
	}

	if err = unaryCheck(a); err != nil {
		return nil, errors.Wrap(err, "Basic checks failed for AddScalar")
	}

	var mem, memB cu.DevicePtr
	var size int64
	if mem, size, retVal, err = e.opMem(a, opts...); err != nil {
		return nil, errors.Wrap(err, "Unable to perform Add")
	}
	memB = cu.DevicePtr(bMem.Uintptr())
	if !leftTensor {
		mem, memB = memB, mem
	}

	debug.Logf("CUDADO %q, Mem: %v size %v, args %v", name, mem, size)
	debug.Logf("LaunchKernel Params. mem: %v. Size %v", mem, size)
	if err = e.Call(name, int(size), unsafe.Pointer(&mem), unsafe.Pointer(&memB), unsafe.Pointer(&size)); err != nil {
		err = errors.Wrap(err, "Unable to perform engine.Add - CUDA LaunchAndSync failed.")
	}
	return
}

// Sub implements tensor.Suber. It does not support safe or increment operation options and will return an error if those options are passed in.
func (e *Engine) Sub(a tensor.Tensor, b tensor.Tensor, opts ...tensor.FuncOpt) (retVal tensor.Tensor, err error) {
	name := constructBinName2(a, b, "sub")

	if err = binaryCheck(a, b); err != nil {
		return nil, errors.Wrap(err, "Basic checks failed for Sub")
	}

	var mem, memB cu.DevicePtr
	var size int64
	if mem, size, retVal, err = e.opMem(a, opts...); err != nil {
		return nil, errors.Wrap(err, "Unable to perform Sub")
	}
	memB = cu.DevicePtr(b.Uintptr())

	debug.Logf("CUDADO %q, Mem: %v MemB: %v size %v", name, mem, memB, size)
	debug.Logf("LaunchKernel Params. mem: %v. Size %v", mem, size)
	if err = e.Call(name, int(size), unsafe.Pointer(&mem), unsafe.Pointer(&memB), unsafe.Pointer(&size)); err != nil {
		err = errors.Wrap(err, "Unable to perform engine.Sub - CUDA LaunchAndSync failed.")
	}
	return
}

// SubScalar implements tensor.Suber. It does not support safe or increment operation options and will return an error if those options are passed in.
func (e *Engine) SubScalar(a tensor.Tensor, b interface{}, leftTensor bool, opts ...tensor.FuncOpt) (retVal tensor.Tensor, err error) {
	name := constructBinName1(a, leftTensor, "sub")

	var bMem tensor.Memory
	var ok bool
	if bMem, ok = b.(tensor.Memory); !ok {
		return nil, errors.Errorf("b has to be a tensor.Memory. Got %T instead", b)
	}

	if err = unaryCheck(a); err != nil {
		return nil, errors.Wrap(err, "Basic checks failed for SubScalar")
	}

	var mem, memB cu.DevicePtr
	var size int64
	if mem, size, retVal, err = e.opMem(a, opts...); err != nil {
		return nil, errors.Wrap(err, "Unable to perform Sub")
	}
	memB = cu.DevicePtr(bMem.Uintptr())
	if !leftTensor {
		mem, memB = memB, mem
	}

	debug.Logf("CUDADO %q, Mem: %v size %v, args %v", name, mem, size)
	debug.Logf("LaunchKernel Params. mem: %v. Size %v", mem, size)
	if err = e.Call(name, int(size), unsafe.Pointer(&mem), unsafe.Pointer(&memB), unsafe.Pointer(&size)); err != nil {
		err = errors.Wrap(err, "Unable to perform engine.Sub - CUDA LaunchAndSync failed.")
	}
	return
}

// Mul implements tensor.Muler. It does not support safe or increment operation options and will return an error if those options are passed in.
func (e *Engine) Mul(a tensor.Tensor, b tensor.Tensor, opts ...tensor.FuncOpt) (retVal tensor.Tensor, err error) {
	name := constructBinName2(a, b, "mul")

	if err = binaryCheck(a, b); err != nil {
		return nil, errors.Wrap(err, "Basic checks failed for Mul")
	}

	var mem, memB cu.DevicePtr
	var size int64
	if mem, size, retVal, err = e.opMem(a, opts...); err != nil {
		return nil, errors.Wrap(err, "Unable to perform Mul")
	}
	memB = cu.DevicePtr(b.Uintptr())

	debug.Logf("CUDADO %q, Mem: %v MemB: %v size %v", name, mem, memB, size)
	debug.Logf("LaunchKernel Params. mem: %v. Size %v", mem, size)
	if err = e.Call(name, int(size), unsafe.Pointer(&mem), unsafe.Pointer(&memB), unsafe.Pointer(&size)); err != nil {
		err = errors.Wrap(err, "Unable to perform engine.Mul - CUDA LaunchAndSync failed.")
	}
	return
}

// MulScalar implements tensor.Muler. It does not support safe or increment operation options and will return an error if those options are passed in.
func (e *Engine) MulScalar(a tensor.Tensor, b interface{}, leftTensor bool, opts ...tensor.FuncOpt) (retVal tensor.Tensor, err error) {
	name := constructBinName1(a, leftTensor, "mul")

	var bMem tensor.Memory
	var ok bool
	if bMem, ok = b.(tensor.Memory); !ok {
		return nil, errors.Errorf("b has to be a tensor.Memory. Got %T instead", b)
	}

	if err = unaryCheck(a); err != nil {
		return nil, errors.Wrap(err, "Basic checks failed for MulScalar")
	}

	var mem, memB cu.DevicePtr
	var size int64
	if mem, size, retVal, err = e.opMem(a, opts...); err != nil {
		return nil, errors.Wrap(err, "Unable to perform Mul")
	}
	memB = cu.DevicePtr(bMem.Uintptr())
	if !leftTensor {
		mem, memB = memB, mem
	}

	debug.Logf("CUDADO %q, Mem: %v size %v, args %v", name, mem, size)
	debug.Logf("LaunchKernel Params. mem: %v. Size %v", mem, size)
	if err = e.Call(name, int(size), unsafe.Pointer(&mem), unsafe.Pointer(&memB), unsafe.Pointer(&size)); err != nil {
		err = errors.Wrap(err, "Unable to perform engine.Mul - CUDA LaunchAndSync failed.")
	}
	return
}

// Div implements tensor.Diver. It does not support safe or increment operation options and will return an error if those options are passed in.
func (e *Engine) Div(a tensor.Tensor, b tensor.Tensor, opts ...tensor.FuncOpt) (retVal tensor.Tensor, err error) {
	name := constructBinName2(a, b, "div")

	if err = binaryCheck(a, b); err != nil {
		return nil, errors.Wrap(err, "Basic checks failed for Div")
	}

	var mem, memB cu.DevicePtr
	var size int64
	if mem, size, retVal, err = e.opMem(a, opts...); err != nil {
		return nil, errors.Wrap(err, "Unable to perform Div")
	}
	memB = cu.DevicePtr(b.Uintptr())

	debug.Logf("CUDADO %q, Mem: %v MemB: %v size %v", name, mem, memB, size)
	debug.Logf("LaunchKernel Params. mem: %v. Size %v", mem, size)
	if err = e.Call(name, int(size), unsafe.Pointer(&mem), unsafe.Pointer(&memB), unsafe.Pointer(&size)); err != nil {
		err = errors.Wrap(err, "Unable to perform engine.Div - CUDA LaunchAndSync failed.")
	}
	return
}

// DivScalar implements tensor.Diver. It does not support safe or increment operation options and will return an error if those options are passed in.
func (e *Engine) DivScalar(a tensor.Tensor, b interface{}, leftTensor bool, opts ...tensor.FuncOpt) (retVal tensor.Tensor, err error) {
	name := constructBinName1(a, leftTensor, "div")

	var bMem tensor.Memory
	var ok bool
	if bMem, ok = b.(tensor.Memory); !ok {
		return nil, errors.Errorf("b has to be a tensor.Memory. Got %T instead", b)
	}

	if err = unaryCheck(a); err != nil {
		return nil, errors.Wrap(err, "Basic checks failed for DivScalar")
	}

	var mem, memB cu.DevicePtr
	var size int64
	if mem, size, retVal, err = e.opMem(a, opts...); err != nil {
		return nil, errors.Wrap(err, "Unable to perform Div")
	}
	memB = cu.DevicePtr(bMem.Uintptr())
	if !leftTensor {
		mem, memB = memB, mem
	}

	debug.Logf("CUDADO %q, Mem: %v size %v, args %v", name, mem, size)
	debug.Logf("LaunchKernel Params. mem: %v. Size %v", mem, size)
	if err = e.Call(name, int(size), unsafe.Pointer(&mem), unsafe.Pointer(&memB), unsafe.Pointer(&size)); err != nil {
		err = errors.Wrap(err, "Unable to perform engine.Div - CUDA LaunchAndSync failed.")
	}
	return
}

// Pow implements tensor.Power. It does not support safe or increment operation options and will return an error if those options are passed in.
func (e *Engine) Pow(a tensor.Tensor, b tensor.Tensor, opts ...tensor.FuncOpt) (retVal tensor.Tensor, err error) {
	name := constructBinName2(a, b, "pow")

	if err = binaryCheck(a, b); err != nil {
		return nil, errors.Wrap(err, "Basic checks failed for Pow")
	}

	var mem, memB cu.DevicePtr
	var size int64
	if mem, size, retVal, err = e.opMem(a, opts...); err != nil {
		return nil, errors.Wrap(err, "Unable to perform Pow")
	}
	memB = cu.DevicePtr(b.Uintptr())

	debug.Logf("CUDADO %q, Mem: %v MemB: %v size %v", name, mem, memB, size)
	debug.Logf("LaunchKernel Params. mem: %v. Size %v", mem, size)
	if err = e.Call(name, int(size), unsafe.Pointer(&mem), unsafe.Pointer(&memB), unsafe.Pointer(&size)); err != nil {
		err = errors.Wrap(err, "Unable to perform engine.Pow - CUDA LaunchAndSync failed.")
	}
	return
}

// PowScalar implements tensor.Power. It does not support safe or increment operation options and will return an error if those options are passed in.
func (e *Engine) PowScalar(a tensor.Tensor, b interface{}, leftTensor bool, opts ...tensor.FuncOpt) (retVal tensor.Tensor, err error) {
	name := constructBinName1(a, leftTensor, "pow")

	var bMem tensor.Memory
	var ok bool
	if bMem, ok = b.(tensor.Memory); !ok {
		return nil, errors.Errorf("b has to be a tensor.Memory. Got %T instead", b)
	}

	if err = unaryCheck(a); err != nil {
		return nil, errors.Wrap(err, "Basic checks failed for PowScalar")
	}

	var mem, memB cu.DevicePtr
	var size int64
	if mem, size, retVal, err = e.opMem(a, opts...); err != nil {
		return nil, errors.Wrap(err, "Unable to perform Pow")
	}
	memB = cu.DevicePtr(bMem.Uintptr())
	if !leftTensor {
		mem, memB = memB, mem
	}

	debug.Logf("CUDADO %q, Mem: %v size %v, args %v", name, mem, size)
	debug.Logf("LaunchKernel Params. mem: %v. Size %v", mem, size)
	if err = e.Call(name, int(size), unsafe.Pointer(&mem), unsafe.Pointer(&memB), unsafe.Pointer(&size)); err != nil {
		err = errors.Wrap(err, "Unable to perform engine.Pow - CUDA LaunchAndSync failed.")
	}
	return
}

// Mod implements tensor.Moder. It does not support safe or increment operation options and will return an error if those options are passed in.
func (e *Engine) Mod(a tensor.Tensor, b tensor.Tensor, opts ...tensor.FuncOpt) (retVal tensor.Tensor, err error) {
	name := constructBinName2(a, b, "mod")

	if err = binaryCheck(a, b); err != nil {
		return nil, errors.Wrap(err, "Basic checks failed for Mod")
	}

	var mem, memB cu.DevicePtr
	var size int64
	if mem, size, retVal, err = e.opMem(a, opts...); err != nil {
		return nil, errors.Wrap(err, "Unable to perform Mod")
	}
	memB = cu.DevicePtr(b.Uintptr())

	debug.Logf("CUDADO %q, Mem: %v MemB: %v size %v", name, mem, memB, size)
	debug.Logf("LaunchKernel Params. mem: %v. Size %v", mem, size)
	if err = e.Call(name, int(size), unsafe.Pointer(&mem), unsafe.Pointer(&memB), unsafe.Pointer(&size)); err != nil {
		err = errors.Wrap(err, "Unable to perform engine.Mod - CUDA LaunchAndSync failed.")
	}
	return
}

// ModScalar implements tensor.Moder. It does not support safe or increment operation options and will return an error if those options are passed in.
func (e *Engine) ModScalar(a tensor.Tensor, b interface{}, leftTensor bool, opts ...tensor.FuncOpt) (retVal tensor.Tensor, err error) {
	name := constructBinName1(a, leftTensor, "mod")

	var bMem tensor.Memory
	var ok bool
	if bMem, ok = b.(tensor.Memory); !ok {
		return nil, errors.Errorf("b has to be a tensor.Memory. Got %T instead", b)
	}

	if err = unaryCheck(a); err != nil {
		return nil, errors.Wrap(err, "Basic checks failed for ModScalar")
	}

	var mem, memB cu.DevicePtr
	var size int64
	if mem, size, retVal, err = e.opMem(a, opts...); err != nil {
		return nil, errors.Wrap(err, "Unable to perform Mod")
	}
	memB = cu.DevicePtr(bMem.Uintptr())
	if !leftTensor {
		mem, memB = memB, mem
	}

	debug.Logf("CUDADO %q, Mem: %v size %v, args %v", name, mem, size)
	debug.Logf("LaunchKernel Params. mem: %v. Size %v", mem, size)
	if err = e.Call(name, int(size), unsafe.Pointer(&mem), unsafe.Pointer(&memB), unsafe.Pointer(&size)); err != nil {
		err = errors.Wrap(err, "Unable to perform engine.Mod - CUDA LaunchAndSync failed.")
	}
	return
}
